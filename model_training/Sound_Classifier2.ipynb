{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = '/home/karan/JupyterKaran/SoundClassifier/UrbanSound8K/metadata//UrbanSound8K.csv'  # Update this path\n",
    "audio_path = '/home/karan/JupyterKaran/SoundClassifier/UrbanSound8K/audio'   # Update this path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectrogram_features(metadata_path, audio_path, img_size=(64, 64)):\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    emergency_classes = ['car_horn', 'siren']\n",
    "    metadata['emergency'] = metadata['class'].apply(lambda x: 1 if x in emergency_classes else 0)\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for index, row in metadata.iterrows():\n",
    "        file_path = os.path.join(audio_path, f\"fold{row['fold']}\", row['slice_file_name'])\n",
    "        \n",
    "        try:\n",
    "            audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "            \n",
    "            # Pad/crop audio to ensure consistent duration\n",
    "            target_samples = int(1.0 * sample_rate)  # 1-second audio\n",
    "            if len(audio) < target_samples:\n",
    "                audio = np.pad(audio, (0, target_samples - len(audio)), mode='constant')\n",
    "            else:\n",
    "                audio = audio[:target_samples]\n",
    "            \n",
    "            # Generate spectrogram\n",
    "            spect = librosa.feature.melspectrogram(\n",
    "                y=audio,\n",
    "                sr=sample_rate,\n",
    "                n_fft=2048,\n",
    "                hop_length=512,\n",
    "                n_mels=img_size[0]\n",
    "            )\n",
    "            spect = librosa.power_to_db(spect, ref=np.max)\n",
    "            \n",
    "            # Resize to exact dimensions (64, 64)\n",
    "            spect = cv2.resize(spect, (img_size[1], img_size[0]))\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            spect = (spect - spect.min()) / (spect.max() - spect.min())\n",
    "            \n",
    "            features.append(spect)\n",
    "            labels.append(row['emergency'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays with explicit shape\n",
    "    features = np.array(features).reshape(-1, img_size[0], img_size[1], 1)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extract_spectrogram_features(metadata_path, audio_path, img_size=(64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1, 64, 64, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 16:22:41.793066: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-04 16:22:41.793641: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-04 16:22:41.795899: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-04 16:22:41.800928: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743763961.809870  120732 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743763961.812221  120732 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743763961.818740  120732 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743763961.818748  120732 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743763961.818749  120732 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743763961.818750  120732 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-04 16:22:41.821452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/karan/JupyterKaran/SoundClassifier/envr/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-04-04 16:22:43.512875: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,952</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m320\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m589,952\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚           \u001b[38;5;34m129\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">682,753</span> (2.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m682,753\u001b[0m (2.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">682,753</span> (2.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m682,753\u001b[0m (2.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    # Convolutional layers\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9921 - loss: 0.0251 - val_accuracy: 0.9725 - val_loss: 0.2219\n",
      "Epoch 2/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9987 - loss: 0.0046 - val_accuracy: 0.9697 - val_loss: 0.2419\n",
      "Epoch 3/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9972 - loss: 0.0081 - val_accuracy: 0.9651 - val_loss: 0.1967\n",
      "Epoch 4/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - accuracy: 0.9893 - loss: 0.0304 - val_accuracy: 0.9748 - val_loss: 0.2184\n",
      "Epoch 5/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9994 - loss: 0.0020 - val_accuracy: 0.9685 - val_loss: 0.2298\n",
      "Epoch 6/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9972 - loss: 0.0082 - val_accuracy: 0.9668 - val_loss: 0.2950\n",
      "Epoch 7/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9982 - loss: 0.0108 - val_accuracy: 0.9725 - val_loss: 0.2161\n",
      "Epoch 8/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step - accuracy: 0.9988 - loss: 0.0057 - val_accuracy: 0.9725 - val_loss: 0.2935\n",
      "Epoch 9/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 8.7203e-04 - val_accuracy: 0.9720 - val_loss: 0.2423\n",
      "Epoch 10/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.9679 - val_loss: 0.2133\n",
      "Epoch 11/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9909 - loss: 0.0261 - val_accuracy: 0.9685 - val_loss: 0.2749\n",
      "Epoch 12/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9936 - loss: 0.0238 - val_accuracy: 0.9708 - val_loss: 0.2136\n",
      "Epoch 13/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step - accuracy: 0.9986 - loss: 0.0042 - val_accuracy: 0.9731 - val_loss: 0.2341\n",
      "Epoch 14/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9999 - loss: 6.9982e-04 - val_accuracy: 0.9725 - val_loss: 0.2904\n",
      "Epoch 15/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 7.0226e-04 - val_accuracy: 0.9708 - val_loss: 0.2547\n",
      "Epoch 16/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9944 - loss: 0.0183 - val_accuracy: 0.9702 - val_loss: 0.2890\n",
      "Epoch 17/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9959 - loss: 0.0116 - val_accuracy: 0.9697 - val_loss: 0.3276\n",
      "Epoch 18/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9936 - loss: 0.0240 - val_accuracy: 0.9576 - val_loss: 0.2671\n",
      "Epoch 19/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9932 - loss: 0.0250 - val_accuracy: 0.9720 - val_loss: 0.2632\n",
      "Epoch 20/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step - accuracy: 0.9923 - loss: 0.0268 - val_accuracy: 0.9720 - val_loss: 0.2220\n",
      "Epoch 21/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9977 - loss: 0.0063 - val_accuracy: 0.9737 - val_loss: 0.2663\n",
      "Epoch 22/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step - accuracy: 0.9995 - loss: 0.0016 - val_accuracy: 0.9742 - val_loss: 0.2534\n",
      "Epoch 23/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 5.7068e-04 - val_accuracy: 0.9737 - val_loss: 0.2592\n",
      "Epoch 24/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 39ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 0.9645 - val_loss: 0.2863\n",
      "Epoch 25/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 43ms/step - accuracy: 0.9990 - loss: 0.0041 - val_accuracy: 0.9720 - val_loss: 0.3027\n",
      "Epoch 26/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 42ms/step - accuracy: 0.9992 - loss: 0.0020 - val_accuracy: 0.9697 - val_loss: 0.3276\n",
      "Epoch 27/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.9936 - loss: 0.0170 - val_accuracy: 0.9639 - val_loss: 0.2242\n",
      "Epoch 28/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.9976 - loss: 0.0088 - val_accuracy: 0.9702 - val_loss: 0.3403\n",
      "Epoch 29/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 43ms/step - accuracy: 0.9970 - loss: 0.0101 - val_accuracy: 0.9702 - val_loss: 0.3023\n",
      "Epoch 30/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.9948 - loss: 0.0137 - val_accuracy: 0.9634 - val_loss: 0.3110\n",
      "Epoch 31/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - accuracy: 0.9960 - loss: 0.0125 - val_accuracy: 0.9731 - val_loss: 0.3060\n",
      "Epoch 32/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - accuracy: 0.9975 - loss: 0.0064 - val_accuracy: 0.9668 - val_loss: 0.2621\n",
      "Epoch 33/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - accuracy: 0.9974 - loss: 0.0071 - val_accuracy: 0.9679 - val_loss: 0.3432\n",
      "Epoch 34/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 0.9679 - val_loss: 0.2676\n",
      "Epoch 35/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9956 - loss: 0.0187 - val_accuracy: 0.9691 - val_loss: 0.3354\n",
      "Epoch 36/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - accuracy: 0.9975 - loss: 0.0057 - val_accuracy: 0.9737 - val_loss: 0.3196\n",
      "Epoch 37/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 43ms/step - accuracy: 0.9991 - loss: 0.0013 - val_accuracy: 0.9708 - val_loss: 0.3270\n",
      "Epoch 38/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - accuracy: 0.9994 - loss: 0.0015 - val_accuracy: 0.9697 - val_loss: 0.3191\n",
      "Epoch 39/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - accuracy: 0.9980 - loss: 0.0054 - val_accuracy: 0.9697 - val_loss: 0.2784\n",
      "Epoch 40/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - accuracy: 0.9997 - loss: 0.0025 - val_accuracy: 0.9634 - val_loss: 0.3484\n",
      "Epoch 41/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - accuracy: 0.9903 - loss: 0.0330 - val_accuracy: 0.9657 - val_loss: 0.2749\n",
      "Epoch 42/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - accuracy: 0.9954 - loss: 0.0142 - val_accuracy: 0.9645 - val_loss: 0.3094\n",
      "Epoch 43/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step - accuracy: 0.9946 - loss: 0.0219 - val_accuracy: 0.9691 - val_loss: 0.2181\n",
      "Epoch 44/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9986 - loss: 0.0043 - val_accuracy: 0.9754 - val_loss: 0.2303\n",
      "Epoch 45/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.9989 - loss: 0.0022 - val_accuracy: 0.9725 - val_loss: 0.2795\n",
      "Epoch 46/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - accuracy: 0.9996 - loss: 0.0020 - val_accuracy: 0.9720 - val_loss: 0.2593\n",
      "Epoch 47/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 48ms/step - accuracy: 0.9998 - loss: 9.6020e-04 - val_accuracy: 0.9708 - val_loss: 0.3216\n",
      "Epoch 48/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - accuracy: 0.9994 - loss: 0.0017 - val_accuracy: 0.9731 - val_loss: 0.3048\n",
      "Epoch 49/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.9976 - loss: 0.0082 - val_accuracy: 0.9702 - val_loss: 0.2730\n",
      "Epoch 50/50\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 47ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.9662 - val_loss: 0.3370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "model.save('emergency_sound_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9645 - loss: 0.3262\n",
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Predict on new audio\n",
    "def predict_audio(file_path, model, img_size=(64, 64)):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # Generate spectrogram\n",
    "    spect = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    spect = librosa.power_to_db(spect, ref=np.max)\n",
    "    spect = cv2.resize(spect, img_size)\n",
    "    spect = (spect - spect.min()) / (spect.max() - spect.min())\n",
    "    \n",
    "    # Reshape for model\n",
    "    spect = spect.reshape(1, 64, 64, 1)\n",
    "    \n",
    "    # Predict\n",
    "    prob = model.predict(spect)[0][0]\n",
    "    return \"Emergency Sound\" if prob > 0.5 else \"Normal Sound\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted buffer: 22528 samples (1.022 sec)\n",
      "Chunks per buffer: 22\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "# Constants\n",
    "RATE = 22050\n",
    "CHUNK = 1024  # Standard chunk size\n",
    "BUFFER_SECONDS = 1.0\n",
    "\n",
    "# Calculate optimal buffer size\n",
    "CHUNKS_PER_BUFFER = int(np.ceil(RATE * BUFFER_SECONDS / CHUNK))\n",
    "BUFFER_SIZE = CHUNKS_PER_BUFFER * CHUNK  # Now perfectly divisible\n",
    "\n",
    "print(f\"Adjusted buffer: {BUFFER_SIZE} samples ({BUFFER_SIZE/RATE:.3f} sec)\")\n",
    "print(f\"Chunks per buffer: {CHUNKS_PER_BUFFER}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_buffer = np.zeros(BUFFER_SIZE, dtype=np.float32)\n",
    "ptr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format=pyaudio.paFloat32,\n",
    "                channels=1,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "audio_buffer = np.zeros(BUFFER_SIZE, dtype=np.float32)\n",
    "write_ptr = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ™ï¸ Listening for emergency sounds... (Press Ctrl+C to stop)\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ™ï¸ Listening for emergency sounds... (Press Ctrl+C to stop)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.412\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.429\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 1.159\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 1.156\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 1.195\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 1.195\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 1.031\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.373\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.362\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 1.069\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 1.010\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.369\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 1.009\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 1.006\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.766\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 1.010\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.368\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.435\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.582\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.317\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.331\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.406\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.336\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.295\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.461\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.334\n",
      "\n",
      "Buffer full - ready for processing!\n",
      "Max amplitude: 0.390\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        # Read chunk\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        chunk = np.frombuffer(data, dtype=np.float32)\n",
    "        \n",
    "        # Write to buffer\n",
    "        audio_buffer[write_ptr:write_ptr+CHUNK] = chunk\n",
    "        write_ptr += CHUNK\n",
    "        \n",
    "        # Process when buffer is full\n",
    "        if write_ptr >= BUFFER_SIZE:\n",
    "            print(\"\\nBuffer full - ready for processing!\")\n",
    "            print(f\"Max amplitude: {np.max(audio_buffer):.3f}\")\n",
    "            \n",
    "            # Reset for next cycle\n",
    "            write_ptr = 0\n",
    "            audio_buffer.fill(0)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "print(model.input_shape)  # Should be (None, 64, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: sof-hda-dsp: - (hw:0,0) (Inputs: 2)\n",
      "1: sof-hda-dsp: - (hw:0,3) (Inputs: 0)\n",
      "2: sof-hda-dsp: - (hw:0,4) (Inputs: 0)\n",
      "3: sof-hda-dsp: - (hw:0,5) (Inputs: 0)\n",
      "4: sof-hda-dsp: - (hw:0,6) (Inputs: 2)\n",
      "5: sof-hda-dsp: - (hw:0,7) (Inputs: 2)\n",
      "6: sof-hda-dsp: - (hw:0,31) (Inputs: 0)\n",
      "7: sysdefault (Inputs: 128)\n",
      "8: pipewire (Inputs: 64)\n",
      "9: dmix (Inputs: 0)\n",
      "10: default (Inputs: 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.front\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround40\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround41\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround50\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround51\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround71\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.hdmi\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.hdmi\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "p = pyaudio.PyAudio()\n",
    "for i in range(p.get_device_count()):\n",
    "    dev = p.get_device_info_by_index(i)\n",
    "    print(f\"{i}: {dev['name']} (Inputs: {dev['maxInputChannels']})\")\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available devices:\n",
      "0: sof-hda-dsp: - (hw:0,0)\n",
      "1: sof-hda-dsp: - (hw:0,3)\n",
      "2: sof-hda-dsp: - (hw:0,4)\n",
      "3: sof-hda-dsp: - (hw:0,5)\n",
      "4: sof-hda-dsp: - (hw:0,7)\n",
      "5: sof-hda-dsp: - (hw:0,31)\n",
      "6: sysdefault\n",
      "7: pipewire\n",
      "8: dmix\n",
      "9: default\n",
      "\n",
      "Speak into your microphone...\n",
      "Input level: 0.0914\n",
      "Audio detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.front\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround40\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround41\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround50\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround51\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround71\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.hdmi\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.hdmi\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "\n",
    "# Audio settings\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paFloat32\n",
    "CHANNELS = 1\n",
    "RATE = 44100  # Try higher rate if 22050 fails\n",
    "DEVICE_INDEX = None  # Set to your mic index if needed\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Print available devices again\n",
    "print(\"\\nAvailable devices:\")\n",
    "for i in range(p.get_device_count()):\n",
    "    dev = p.get_device_info_by_index(i)\n",
    "    print(f\"{i}: {dev['name']}\")\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                input_device_index=DEVICE_INDEX,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"\\nSpeak into your microphone...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        audio = np.frombuffer(data, dtype=np.float32)\n",
    "        volume = np.abs(audio).mean()\n",
    "        print(f\"Input level: {volume:.4f}\", end='\\r')\n",
    "        if volume > 0.01:  # If you see this, audio is working\n",
    "            print(\"\\nAudio detected!\")\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available devices:\n",
      "0: sof-hda-dsp: - (hw:0,0)\n",
      "1: sof-hda-dsp: - (hw:0,3)\n",
      "2: sof-hda-dsp: - (hw:0,4)\n",
      "3: sof-hda-dsp: - (hw:0,5)\n",
      "4: sof-hda-dsp: - (hw:0,7)\n",
      "5: sof-hda-dsp: - (hw:0,31)\n",
      "6: sysdefault\n",
      "7: pipewire\n",
      "8: dmix\n",
      "9: default\n",
      "\n",
      "Speak into your microphone...\n",
      "Input level: 0.2977\n",
      "Audio detected!\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "\n",
    "# Audio settings\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paFloat32\n",
    "CHANNELS = 1\n",
    "RATE = 44100  # Try higher rate if 22050 fails\n",
    "DEVICE_INDEX = None  # Set to your mic index if needed\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Print available devices again\n",
    "print(\"\\nAvailable devices:\")\n",
    "for i in range(p.get_device_count()):\n",
    "    dev = p.get_device_info_by_index(i)\n",
    "    print(f\"{i}: {dev['name']}\")\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                input_device_index=DEVICE_INDEX,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"\\nSpeak into your microphone...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        audio = np.frombuffer(data, dtype=np.float32)\n",
    "        volume = np.abs(audio).mean()\n",
    "        print(f\"Input level: {volume:.4f}\", end='\\r')\n",
    "        if volume > 0.01:  # If you see this, audio is working\n",
    "            print(\"\\nAudio detected!\")\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circular buffer implementation\n",
    "audio_buffer = np.zeros(BUFFER_SIZE, dtype=np.float32)\n",
    "write_ptr = 0\n",
    "chunk_counter = 0\n",
    "\n",
    "def process_audio(buffer):\n",
    "    # Create spectrogram\n",
    "    spect = librosa.feature.melspectrogram(y=buffer, sr=RATE, n_mels=64)\n",
    "    spect = librosa.power_to_db(spect, ref=np.max)\n",
    "    spect = cv2.resize(spect, (64, 64))\n",
    "    \n",
    "    # Safe normalization\n",
    "    spect = np.nan_to_num(spect)\n",
    "    if np.ptp(spect) > 0:\n",
    "        spect = (spect - spect.min()) / np.ptp(spect)\n",
    "    else:\n",
    "        spect = np.zeros((64, 64))\n",
    "    return np.expand_dims(spect, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "RATE = 22050\n",
    "CHUNK = 1024  # Must be power of 2 (512, 1024, 2048)\n",
    "BUFFER_SECONDS = 1.0\n",
    "BUFFER_SIZE = int(RATE * BUFFER_SECONDS)\n",
    "THRESHOLD = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Error: [Errno -9988] Stream closed\n",
      "\n",
      "Stopping...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        try:\n",
    "            # Read audio chunk\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            chunk = np.frombuffer(data, dtype=np.float32)\n",
    "            \n",
    "            # Write to circular buffer\n",
    "            if write_ptr + CHUNK <= BUFFER_SIZE:\n",
    "                audio_buffer[write_ptr:write_ptr+CHUNK] = chunk\n",
    "            else:\n",
    "                remaining = BUFFER_SIZE - write_ptr\n",
    "                audio_buffer[write_ptr:] = chunk[:remaining]\n",
    "                audio_buffer[:CHUNK-remaining] = chunk[remaining:]\n",
    "            \n",
    "            write_ptr = (write_ptr + CHUNK) % BUFFER_SIZE\n",
    "            chunk_counter += 1\n",
    "            \n",
    "            print(f\"{RATE} samples/sec Ã· {CHUNK} samples/chunk = {RATE/CHUNK:.2f} chunks/sec\")\n",
    "            print(f\"Exact buffer: {BUFFER_SIZE} samples = {BUFFER_SIZE/CHUNK} chunks\")\n",
    "            # Process every full buffer cycle\n",
    "            if chunk_counter >= BUFFER_SIZE // CHUNK:\n",
    "                spect = process_audio(audio_buffer)\n",
    "                prob = model.predict(spect[np.newaxis, ...], verbose=0)[0][0]\n",
    "                \n",
    "                if prob > THRESHOLD:\n",
    "                    print(f\"ğŸš¨ EMERGENCY ({prob:.0%} confidence)\")\n",
    "                else:\n",
    "                    print(f\"Normal ({prob:.0%})\", end='\\r')\n",
    "                \n",
    "                chunk_counter = 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "            time.sleep(0.1)\n",
    "            # Reset buffer on serious errors\n",
    "            audio_buffer.fill(0)\n",
    "            write_ptr = 0\n",
    "            chunk_counter = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopping...\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 19:43:42.864445: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-11 19:43:42.865006: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-11 19:43:42.868424: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-11 19:43:42.875237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744380822.887226   47566 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744380822.890391   47566 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744380822.899106   47566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744380822.899127   47566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744380822.899129   47566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744380822.899130   47566 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-11 19:43:42.902554: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-11 19:43:44.366113: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp37usm71d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp37usm71d/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp37usm71d'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 64, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  139037023710352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139037023711120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139037023713040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139037023710736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139037023714384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139037023715152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139037023714768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139036951216976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139036951218128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139036951219280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1744380824.686525   47566 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1744380824.686541   47566 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-04-11 19:43:44.686906: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp37usm71d\n",
      "2025-04-11 19:43:44.687281: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-04-11 19:43:44.687287: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp37usm71d\n",
      "I0000 00:00:1744380824.691403   47566 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "2025-04-11 19:43:44.692189: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-04-11 19:43:44.724181: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp37usm71d\n",
      "2025-04-11 19:43:44.735274: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 48370 microseconds.\n",
      "2025-04-11 19:43:44.775641: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('emergency_sound_model.h5')\n",
    "\n",
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Apply optimizations (optional but recommended)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# For further size reduction and speed (optional)\n",
    "# converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('emergency_sound_classifier.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
